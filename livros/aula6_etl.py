# -*- coding: utf-8 -*-
"""Aula6_ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KmsIe0XeSpZblZN3F99p4VZIkvWP0-q-
"""

# Aula 6
# correção do exercicio
# tirar duvidas
# começar transformação dos dados

import pandas as pd
import numpy as np

import requests
from bs4 import BeautifulSoup

# exercicio
# ir na pagina http://books.toscrape.com
# fazer o web scrapinf com find-all para selecionar o
# pegar todos os  elementos article no padrão class="product_pod"
    # for em cima dessa lista
       # for l in lista
           #n = l.find('h3')
           #x = n.find('a').title
           #p = l.find
          #.....
          #listatupla.append
# o nome do livro deve ser o atributo tittle do elemento <a>
# busca.title
# preco é uma div e o padrão class="product_price" ( texto)
# disponibilidade <p class="instock availability">
# Salvar o resultado num data frame
# salvar o data frame em arquivo

#1
re = requests.get('http://books.toscrape.com').text

#2
# fazer o web scraping com find-all para selecionar o
# pegar todos os  elementos article no padrão class="product_pod"

soup = BeautifulSoup(re,'html5lib')

caixa = soup.find_all('article',class_ = "product_pod")
# retorna uma lista de resultados
#para acesar elemento por elemento da lista
# precisa do for

result = []
for c in caixa:
  # 3
  #o nome do livro deve ser o atributo tittle do elemento <a>
  t = c.find('h3')
  nome = t.find('a').get('title')

  #4
  # preco é uma div e o padrão class="product_price" ( texto)
  #<p class="price_color">£51.77</p>
  preco = c.find('p', class_="price_color").text
  #5
  # disponibilidade <p class="instock availability">
  disp = c.find('p', class_ = "instock availability").text
  result.append((nome,preco,disp))

### maira for
ListaTuplas = []

for i in caixa:
    # Encontrar a tag <h3> dentro do artigo que contém o título do livro
    title_tag = i.find('h3').find('a')
    # Extrair o texto do título
    title = title_tag['title']
    # Encontrar a tag <div> com a classe 'product_price'
    price_tag = i.find('div', class_='product_price')
    # Extrair o texto do preço
    price = price_tag.find('p', class_='price_color').get_text(strip=True)
    # Encontrar a tag <p> com a classe 'instock availability'
    availability_tag = i.find('p', class_='instock availability')
    # Extrair o texto da disponibilidade
    availability = availability_tag.get_text(strip=True)
    ListaTuplas.append((title, price, availability))

resultDf = pd.DataFrame(ListaTuplas, columns=['Título', 'Preço', 'Disponibilidade'])

resultDf.head()

len(result)
result

# 6
#Salvar o resultado num data frame
dfLivro = pd.DataFrame(result, columns =  ['livro', 'preco', 'disp'])
dfLivro.head()

#7
# salvar o data frame em arquivo
dfLivro.to_csv("livro.csv", index = False)

# Exercicio Amazon
# Ir no site
#https://web.archive.org/web/20160616183209/http://www.amazon.com.br/gp/bestsellers/books/13983231011
# pegar todos os livros listados neles
# padrão div class="zg_itemImmersion"
## pegar nome do livro
## nome do autor
## preço do livro

## salvar em data Frame
## salvar data Frame em arquivo

url1 = 'https://web.archive.org/web/20160616183209/http://www.amazon.com.br/gp/bestsellers/books/13983231011'
req = requests.get(url1)
print (req)
soup = BeautifulSoup(req.text, 'html.parser')

# pegar todos os livros listados neles
# padrão div class="zg_itemImmersion"
livros = soup('div', class_ = "zg_itemImmersion")

# lista contendo os resultados
result = []

# for variavelControle in Lista:
for livro in livros:
  titulo = livro.find('div', class_ ="zg_title").find('a').text
  autor = livro.find('div', class_ = "zg_byline").text
  preco = livro.find('strong',class_ ='price').text
  result.append((titulo,autor,preco))

print(result)

## salvar em data Frame
dfAmazon = pd.DataFrame(result,
                        columns = ['titulo','autor','preco'])
dfAmazon.head()

## salvar data Frame em arquivo
dfAmazon.to_csv("Amazon_livros.csv")

## como dica de portifolio façam extração por paginação do exercicio anterior
## nesse site https://web.archive.org/web/20160616183209/
## tem sites que da para treinar web scapping que nunca vão perder
## a referencia pois estão historicos
## aquele da lista de artistitas vale a pena de fazer

## falhou
#tabelas = pd.read_html("https://valorinveste.globo.com/cotacoes/")

site = requests.get('https://valorinveste.globo.com/cotacoes/').text
busca = BeautifulSoup(site, 'html5lib')

#falhou
#tabela = busca.find('table')

cabecalho = busca.find('thead', class_ ="vd-table__head" )
print(cabecalho)

td = cabecalho.find_all('td')
len (td)
col = []
for x in td:
  col.append(x.text)

print(td)

conteudo = busca.find('tbody', class_ = "vd-table__body")

linhas = conteudo.find_all('tr')
registros = []
for linha in linhas:
  colunas = linha.find_all('td')
  cont = []
  for coluna in colunas:
    texto = coluna.text
    cont.append(texto)
  registros.append(tuple(cont))

dfBolsa = pd.DataFrame(registros, columns = col)

dfBolsa.head()

rodape = busca.find('tfoot', class_ = "vd-table__footer" ).text
print(rodape)

#falhou
#tabela = "<table> "+ str(cabecalho) + " </table>"

#falhou
#tblDia = pd.read_html(tabela)

#tblDiazzzzzzz